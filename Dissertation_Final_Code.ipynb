{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PFzRVgAaf3t"
      },
      "source": [
        "# **Importing** **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh8FBIfmbjQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db666f19-a10a-4df1-80a7-eff129243aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ],
      "source": [
        "#Importing all required libraries and packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import seaborn as sb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from statsmodels.graphics import tsaplots\n",
        "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
        "import math\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from math import floor\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "from sklearn import model_selection\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "print('Libraries imported.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwN5oQ8Valzz"
      },
      "source": [
        "# **Data Preperation, Pre-Processing, and FE**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data and take a look"
      ],
      "metadata": {
        "id": "rbyUQhq0V6mY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8kdQiuEdF1p"
      },
      "outputs": [],
      "source": [
        "#Read Pollutants Data for Piccadilly\n",
        "PollutantsPicc = pd.read_csv('Piccadilly.csv')\n",
        "PollutantsPicc.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e1rq5iOAhKv"
      },
      "outputs": [],
      "source": [
        "#Read Traffic Data From Piccadilly\n",
        "traffic22 = pd.read_csv('pvr_2016-03-04_1765d(1).csv')\n",
        "#Rename date column to match the date column of the Pollutants Data from Piccadilly\n",
        "traffic22 = traffic22.rename(columns = {\"Sdate\":\"date\"})\n",
        "#Rename Cosit to match the same style as well\n",
        "traffic22['Cosit'].replace('=\"MAC030001146\"', 'MAC030001146', inplace=True)\n",
        "#Remove Channel 1 as it recorded no Traffic volume\n",
        "traffic22 = traffic22[traffic22.LaneDescription != 'Channel 2']\n",
        "traffic22.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read Data from Sharston\n",
        "PollutantsShar = pd.read_csv('Sharston.csv')\n",
        "PollutantsShar.head()"
      ],
      "metadata": {
        "id": "D3qkhtOTCLfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read traffic data for Sharston \n",
        "traffic3 = pd.read_csv('pvr_2015-06-01_2615d.csv')\n",
        "#Rename date column to match pollutants data\n",
        "traffic3 = traffic3.rename(columns = {\"Sdate\":\"date\"})\n",
        "#Remove Channel 1 as it recorded no Traffic volume values\n",
        "traffic3 = traffic3[traffic3.LaneDescription != 'Channel 2']\n",
        "traffic3.head()"
      ],
      "metadata": {
        "id": "kFWegkNzCMFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPNNqTk57k4S"
      },
      "source": [
        "## Data Pre-processing and Feature Engineering Piccadilly - done"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "merged data"
      ],
      "metadata": {
        "id": "PL7klacRBQXK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQPFtVWnlfR3"
      },
      "outputs": [],
      "source": [
        "#merge traffic data and pollutants data on the date column\n",
        "df = pd.merge(traffic22,PollutantsPicc[['date','NO2','wd','ws','temp','longitude', 'latitude']],on='date', how='left')\n",
        "#drop irrelevant variables\n",
        "df = df.drop(['AvgSpeed', 'PmlHGV', 'Flag Text'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Taking a look at the first 5 rows to make sure merging was effictive\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9siHsrjFBD-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for null values\n",
        "df.isnull().mean()"
      ],
      "metadata": {
        "id": "FaI-gG4b9Z9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop null values\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "dEF5Ded49TLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check correlation between variables\n",
        "df[['NO2','Volume','ws','wd','temp']].corr()"
      ],
      "metadata": {
        "id": "eh3R8NPYkYKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#summary statistics for some variables\n",
        "df[['NO2','Volume','ws','wd','temp']].describe()"
      ],
      "metadata": {
        "id": "c_E1WA2vcStn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check variable info including column types\n",
        "df.info()"
      ],
      "metadata": {
        "id": "QWd3z4EoJgs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj4GRql7jK-S"
      },
      "outputs": [],
      "source": [
        "#extract seasonality data drom data column\n",
        "df['date'] =  pd.to_datetime(df['date'])\n",
        "df['Year'] = df['date'].dt.year\n",
        "df['Month'] = df['date'].dt.month\n",
        "df['Hour'] = df['date'].dt.hour\n",
        "df['DayofWeek'] = df['date'].dt.dayofweek\n",
        "df['dayofyear'] = df['date'].dt.dayofyear\n",
        "df['weekofyear'] = df['date'].dt.weekofyear\n",
        "df['quarter'] = df['date'].dt.quarter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create new variables using rolling means function (may not use all)\n",
        "df['NO2_moving_avg'] = df['NO2'].rolling(window=25).mean()\n",
        "df['NO2_moving_avg2'] = df['NO2'].rolling(window=100).mean()\n",
        "df['NO2_moving_avg3'] = df['NO2'].rolling(window=50).mean()\n",
        "df['NO2_moving_avg4'] = df['NO2'].rolling(window=3).mean()\n",
        "df['NO2_moving_avg5'] = df['NO2'].rolling(window=30).mean()"
      ],
      "metadata": {
        "id": "BykH7NI81DhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop any new missing values generating from moving average variables\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "u9xas3UDpTso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform wind direction from degrees to textual direction (for EDA)\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 0, 0)\n",
        "df['wd'] = df['wd'].mask((df['wd'] > 0) & (df['wd'] < 90), 0.5)\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 90, 1)\n",
        "df['wd'] = df['wd'].mask((df['wd'] > 90) & (df['wd'] < 180), 1.5)\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 180, 2)\n",
        "df['wd'] = df['wd'].mask((df['wd'] > 180) & (df['wd'] < 270), 2.5)\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 270, 3)\n",
        "df['wd'] = df['wd'].mask((df['wd'] > 270) & (df['wd'] < 360), 3.5)\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 360, 'North')\n",
        "\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 0, 'North')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 0.5, 'North East')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 1, 'East')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 1.5, 'South East')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 2, 'South')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 2.5, 'South West')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 3, 'West')\n",
        "df['wd'] = df['wd'].mask(df['wd'] == 3.5, 'North West')"
      ],
      "metadata": {
        "id": "kwLIa4UROpiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encode Wind direction\n",
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'wd' column \n",
        "encoder_df = pd.DataFrame(encoder.fit_transform(df[['wd1']]).toarray())\n",
        "\n",
        "#merge one-hot encoded columns back with original DataFrame\n",
        "df = df.join(encoder_df)"
      ],
      "metadata": {
        "id": "XDvER4lBPUPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'Day of week' column \n",
        "df[['ohe1','ohe2','ohe3','oh4','o5','o6','o7']] = encoder.fit_transform(df[['DayofWeek']]).toarray()"
      ],
      "metadata": {
        "id": "7NTiDuNBPZhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pollutants data"
      ],
      "metadata": {
        "id": "dz2_LL9qPLXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drop irrelevant variables\n",
        "PollutantsPicc = PollutantsPicc.drop(['NOXasNO2', 'SO2', 'NV2.5',\n",
        "                                      'V2.5','AT2.5','AP2.5','AT25','AP25',\n",
        "                                      'PM10','RAWPM25'], axis = 1)"
      ],
      "metadata": {
        "id": "rPXD9VI7QgMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for missing values\n",
        "PollutantsPicc.isnull().mean()"
      ],
      "metadata": {
        "id": "ii_PzaPy1G0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correlation for data\n",
        "PollutantsPicc[['NO2', 'ws','wd','temp']].corr()"
      ],
      "metadata": {
        "id": "nlOukuWw-gfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary statistics for data\n",
        "PollutantsPicc[['NO2', 'ws','wd','temp']].describe()"
      ],
      "metadata": {
        "id": "pky1tHxI1SUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform wind direction from degrees to textual direction (for EDA)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 0, 0)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask((PollutantsPicc['wd'] > 0) & (PollutantsPicc['wd'] < 90), 0.5)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 90, 1)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask((PollutantsPicc['wd'] > 90) & (PollutantsPicc['wd'] < 180), 1.5)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 180, 2)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask((PollutantsPicc['wd'] > 180) & (PollutantsPicc['wd'] < 270), 2.5)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 270, 3)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask((PollutantsPicc['wd'] > 270) & (PollutantsPicc['wd'] < 360), 3.5)\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 360, 'North')\n",
        "\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 0, 'North')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 0.5, 'North East')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 1, 'East')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 1.5, 'South East')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 2, 'South')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 2.5, 'South West')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 3, 'West')\n",
        "PollutantsPicc['wd'] = PollutantsPicc['wd'].mask(PollutantsPicc['wd'] == 3.5, 'North West')"
      ],
      "metadata": {
        "id": "6YoGDC7p4LUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encode Wind direction\n",
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'wd' column \n",
        "encoder_df = pd.DataFrame(encoder.fit_transform(PollutantsPicc[['wd']]).toarray())\n",
        "\n",
        "#merge one-hot encoded columns back with original DataFrame\n",
        "PollutantsPicc = PollutantsPicc.join(encoder_df)"
      ],
      "metadata": {
        "id": "yd6eHg4f7J78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check variable info including column types\n",
        "PollutantsPicc.info()"
      ],
      "metadata": {
        "id": "8L1kaEAiKBE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract new features from date column\n",
        "PollutantsPicc['date'] =  pd.to_datetime(PollutantsPicc['date'])\n",
        "PollutantsPicc['Year'] = PollutantsPicc['date'].dt.year\n",
        "PollutantsPicc['Month'] = PollutantsPicc['date'].dt.month\n",
        "PollutantsPicc['Hour'] = PollutantsPicc['date'].dt.hour\n",
        "PollutantsPicc['DayofWeek'] = PollutantsPicc['date'].dt.dayofweek\n",
        "PollutantsPicc['dayofyear'] = PollutantsPicc['date'].dt.dayofyear\n",
        "PollutantsPicc['weekofyear'] = PollutantsPicc['date'].dt.weekofyear\n",
        "PollutantsPicc['quarter'] = PollutantsPicc['date'].dt.quarter\n",
        "#create new variables using rolling means\n",
        "PollutantsPicc['NO2_moving_avg'] = PollutantsPicc['NO2'].rolling(window=3).mean()#3hours\n",
        "PollutantsPicc['NO2_moving_avg2'] = PollutantsPicc['NO2'].rolling(window=25).mean()#24hours\n",
        "PollutantsPicc['NO2_moving_avg3'] = PollutantsPicc['NO2'].rolling(window=50).mean()#~2days\n",
        "PollutantsPicc['NO2_moving_avg4'] = PollutantsPicc['NO2'].rolling(window=100).mean()#~4days\n",
        "PollutantsPicc['NO2_moving_avg5'] = PollutantsPicc['NO2'].rolling(window=200).mean()#~8days"
      ],
      "metadata": {
        "id": "NLn4T0JMQFsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'dayofweek' column \n",
        "PollutantsPicc[['ohe1','ohe2','ohe3','oh4','o5','o6','o7']] = encoder.fit_transform(PollutantsPicc[['DayofWeek']]).toarray()\n",
        "\n",
        "#merge one-hot encoded columns back with original DataFrame\n",
        "#PollutantsPicc = PollutantsPicc.join(encoder_df)"
      ],
      "metadata": {
        "id": "cOVDXmH76fCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop missing values generated from moving average\n",
        "PollutantsPicc = PollutantsPicc.dropna()"
      ],
      "metadata": {
        "id": "XMkJGirbRsnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing and Feature Engineering Sharston"
      ],
      "metadata": {
        "id": "lwnmxC9EaD4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove empty channel 1 data and change the date format to match that of the pollutants data\n",
        "traffic3 = traffic3[traffic3.LaneDescription != 'Channel 2']\n",
        "date_sr = pd.to_datetime(pd.Series(traffic3.date))\n",
        "traffic3.date = change_format = date_sr.dt.strftime('%d/%m/%Y %H:%M')\n",
        " \n",
        "# Print the formatted date\n",
        "print(change_format)"
      ],
      "metadata": {
        "id": "_ZsCm-ebliEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "merged data"
      ],
      "metadata": {
        "id": "A70GLhG6B8aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Megre the data on date\n",
        "df2 = pd.merge(traffic3,PollutantsShar[['date','NO2','wd','ws','temp','longitude', 'latitude']],\n",
        "               on='date', how='left')"
      ],
      "metadata": {
        "id": "rkxwRjh4MBcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whukBWf4r1Yv"
      },
      "outputs": [],
      "source": [
        "#Drop irrelevant null filled variables\n",
        "df2 = df2.drop(['AvgSpeed', 'PmlHGV', 'Flag Text'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#take a look at the first 5 rows to make sure merging was successful\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "ongYdulvKbcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for missing values\n",
        "df2.isnull().mean()"
      ],
      "metadata": {
        "id": "6sIquy7yH1oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop missing values\n",
        "df2 = df2.dropna()"
      ],
      "metadata": {
        "id": "3apW2DIiQzgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check correlation between variables\n",
        "df2.corr()"
      ],
      "metadata": {
        "id": "6eVWWukPKlZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Statistics summary of data\n",
        "df2[['NO2','Volume','ws','wd','temp']].describe()"
      ],
      "metadata": {
        "id": "R6BySzwGE7MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check variable info including column types\n",
        "df2.info()"
      ],
      "metadata": {
        "id": "LhCD8hTsKivu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#New seasonality features extracted\n",
        "df2['date'] =  pd.to_datetime(df2['date'])\n",
        "df2['Year'] = df2['date'].dt.year\n",
        "df2['Month'] = df2['date'].dt.month\n",
        "df2['Hour'] = df2['date'].dt.hour\n",
        "df2['DayofWeek'] = df2['date'].dt.dayofweek\n",
        "df2['dayofyear'] = df2['date'].dt.dayofyear\n",
        "df2['weekofyear'] = df2['date'].dt.weekofyear\n",
        "df2['quarter'] = df2['date'].dt.quarter\n",
        "#rolling means features\n",
        "df2['NO2_moving_avg'] = df2['NO2'].rolling(window=3).mean()\n",
        "df2['NO2_moving_avg2'] = df2['NO2'].rolling(window=25).mean()\n",
        "df2['NO2_moving_avg3'] = df2['NO2'].rolling(window=50).mean()\n",
        "df2['NO2_moving_avg4'] = df2['NO2'].rolling(window=100).mean()"
      ],
      "metadata": {
        "id": "d2CE2ye6Mexc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wind direction from degrees to textual (for EDA)\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 0.1, 0)\n",
        "df2['wd'] = df2['wd'].mask((df2['wd'] > 0.1) & (df2['wd'] < 90), 0.5)\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 90, 1)\n",
        "df2['wd'] = df2['wd'].mask((df2['wd'] > 90) & (df2['wd'] < 180), 1.5)\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 180, 2)\n",
        "df2['wd'] = df2['wd'].mask((df2['wd'] > 180) & (df2['wd'] < 270), 2.5)\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 270, 3)\n",
        "df2['wd'] = df2['wd'].mask((df2['wd'] > 270) & (df2['wd'] < 359.9), 3.5)\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 359.9, 'North')\n",
        "\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 0, 'North')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 0.5, 'North East')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 1, 'East')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 1.5, 'South East')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 2, 'South')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 2.5, 'South West')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 3, 'West')\n",
        "df2['wd'] = df2['wd'].mask(df2['wd'] == 3.5, 'North West')"
      ],
      "metadata": {
        "id": "6gaD_0xiG406"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'wd' column \n",
        "encoder_df = pd.DataFrame(encoder.fit_transform(df2[['wd']]).toarray())\n",
        "\n",
        "#merge one-hot encoded columns back with original DataFrame\n",
        "df2 = df2.join(encoder_df)"
      ],
      "metadata": {
        "id": "kM29K6U8G7aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'dayofweek' column and add them to new columns\n",
        "df2[['ohe1','ohe2','ohe3','oh4','o5','o6','o7']] = encoder.fit_transform(df2[['DayofWeek']]).toarray()"
      ],
      "metadata": {
        "id": "8GZl1OTRG-ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for missing values\n",
        "PollutantsShar.isnull().mean()"
      ],
      "metadata": {
        "id": "vktazqJiIEM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop missing values\n",
        "PollutantsShar = PollutantsShar.dropna()"
      ],
      "metadata": {
        "id": "FXFdq7oRKjX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PollutantsShar.corr()"
      ],
      "metadata": {
        "id": "tVEnUFrvPTN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Statistics summary of data\n",
        "PollutantsShar[['NO2','ws','wd','temp']].describe()"
      ],
      "metadata": {
        "id": "nuGDSCF7F0SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PollutantsShar.info()"
      ],
      "metadata": {
        "id": "vr3xAHn2PWOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract new seasonality features\n",
        "PollutantsShar['date'] =  pd.to_datetime(PollutantsShar['date'])\n",
        "PollutantsShar['Year'] = PollutantsShar['date'].dt.year\n",
        "PollutantsShar['Month'] = PollutantsShar['date'].dt.month\n",
        "PollutantsShar['Hour'] = PollutantsShar['date'].dt.hour\n",
        "PollutantsShar['DayofWeek'] = PollutantsShar['date'].dt.dayofweek\n",
        "PollutantsShar['dayofyear'] = PollutantsShar['date'].dt.dayofyear\n",
        "PollutantsShar['weekofyear'] = PollutantsShar['date'].dt.weekofyear\n",
        "PollutantsShar['quarter'] = PollutantsShar['date'].dt.quarter\n",
        "PollutantsShar['Day'] = PollutantsShar['date'].dt.day\n",
        "#new rollings means variables\n",
        "PollutantsShar['NO2_moving_avg'] = PollutantsShar['NO2'].rolling(window=3).mean()\n",
        "PollutantsShar['NO2_moving_avg2'] = PollutantsShar['NO2'].rolling(window=25).mean()\n",
        "PollutantsShar['NO2_moving_avg3'] = PollutantsShar['NO2'].rolling(window=50).mean()\n",
        "PollutantsShar['NO2_moving_avg4'] = PollutantsShar['NO2'].rolling(window=100).mean()"
      ],
      "metadata": {
        "id": "y7H_kUpQJ5Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wind direction from degrees to textual\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 0, 0)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask((PollutantsShar['wd'] > 0) & (PollutantsShar['wd'] < 90), 0.5)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 90, 1)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask((PollutantsShar['wd'] > 90) & (PollutantsShar['wd'] < 180), 1.5)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 180, 2)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask((PollutantsShar['wd'] > 180) & (PollutantsShar['wd'] < 270), 2.5)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 270, 3)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask((PollutantsShar['wd'] > 270) & (PollutantsShar['wd'] < 360), 3.5)\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 360, 'North')\n",
        "\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 0, 'North')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 0.5, 'North East')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 1, 'East')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 1.5, 'South East')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 2, 'South')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 2.5, 'South West')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 3, 'West')\n",
        "PollutantsShar['wd'] = PollutantsShar['wd'].mask(PollutantsShar['wd'] == 3.5, 'North West')"
      ],
      "metadata": {
        "id": "m8JwOASv4qrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'wd' column \n",
        "encoder_df = pd.DataFrame(encoder.fit_transform(PollutantsShar[['wd']]).toarray())\n",
        "\n",
        "#merge one-hot encoded columns back with original DataFrame\n",
        "PollutantsShar = PollutantsShar.join(encoder_df)"
      ],
      "metadata": {
        "id": "BERM3ufkGgtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating instance of one-hot-encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "#perform one-hot encoding on 'dayofweek' column \n",
        "PollutantsShar[['ohe1','ohe2','ohe3','oh4','o5','o6','o7']] = encoder.fit_transform(PollutantsShar[['DayofWeek']]).toarray()"
      ],
      "metadata": {
        "id": "BlJrixOZGhXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Picc&Shar** **EDA**"
      ],
      "metadata": {
        "id": "xlhjA2YwuBKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "r1ciO44gVhS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig= plt.figure(figsize=(10,5))\n",
        "PollutantsPicc['NO2'].plot(figsize=(10,6))\n",
        "PollutantsPicc['NO2'].rolling(window=200).mean().plot()\n",
        "plt.title('Piccadilly')\n",
        "plt.ylabel('NO2')"
      ],
      "metadata": {
        "id": "3Xk8II1bTiLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig= plt.figure(figsize=(10,5))\n",
        "PollutantsShar['NO2'].plot(figsize=(10,6))\n",
        "PollutantsShar['NO2'].rolling(window=200).mean().plot()\n",
        "plt.ylabel('NO2')\n",
        "plt.title('Sharston')"
      ],
      "metadata": {
        "id": "1azQcyJ_RXhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from windrose import WindroseAxes\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "ax = WindroseAxes.from_ax()\n",
        "ax.bar(PollutantsShar.wd, PollutantsShar.ws, normed=True, opening=0.8, edgecolor='white')\n",
        "ax.set_legend()\n",
        "ax.legend(loc='upper left')\n",
        "ax.set_title('Sharston', loc='right')\n",
        "ax = WindroseAxes.from_ax()\n",
        "ax.bar(PollutantsPicc.wd, PollutantsPicc.ws, normed=True, opening=0.8, edgecolor='white')\n",
        "ax.set_legend()\n",
        "ax.legend(loc='upper left')\n",
        "ax.set_title('Piccadilly', loc='right')"
      ],
      "metadata": {
        "id": "UAYrLtYhiOmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 1, figsize=(12,6))\n",
        "sb.kdeplot(PollutantsPicc.NO2 , bw = 0.5, ax=axes[0], label='Piccadilly')\n",
        "sb.kdeplot(df.Volume , bw = 0.5, ax=axes[1], label='Piccadilly')\n",
        "sb.kdeplot(PollutantsShar.NO2 , bw = 0.5, ax=axes[0], label='Sharston')\n",
        "sb.kdeplot(df2.Volume , bw = 0.5, ax=axes[1], label='Sharston')\n",
        "# labels\n",
        "axes[0].set_xlabel('NO2')\n",
        "axes[0].legend(loc='upper right')\n",
        "axes[1].legend(loc='upper right')\n",
        "axes[1].set_xlabel('Traffic Volume')\n",
        "axes[0].axvline(x=PollutantsPicc.NO2.median(),\n",
        "            color='blue',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "axes[0].axvline(x=PollutantsShar.NO2.median(),\n",
        "            color='orange',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "\n",
        "axes[0].axvline(x=PollutantsPicc.NO2.mean(),\n",
        "            color='violet',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "axes[0].axvline(x=PollutantsShar.NO2.mean(),\n",
        "            color='red',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "\n",
        "\n",
        "axes[1].axvline(x=df.Volume.median(),\n",
        "            color='blue',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "axes[1].axvline(x=df2.Volume.median(),\n",
        "            color='orange',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "\n",
        "axes[1].axvline(x=df.Volume.mean(),\n",
        "            color='violet',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "axes[1].axvline(x=df2.Volume.mean(),\n",
        "            color='red',\n",
        "            ls='--', \n",
        "            lw=2.5)\n",
        "plt.subplots_adjust(hspace=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9_GscvjfIANk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PollutantsPicc.NO2.median())\n",
        "print(PollutantsPicc.NO2.mean())\n",
        "print(df.Volume.mean())\n",
        "print(df.Volume.median())"
      ],
      "metadata": {
        "id": "EXPBEjwdeM_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PollutantsShar.NO2.median())\n",
        "print(PollutantsShar.NO2.mean())\n",
        "print(df2.Volume.mean())\n",
        "print(df2.Volume.median())"
      ],
      "metadata": {
        "id": "rrj7NnGNekv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PollutantsShar.NO2.describe()"
      ],
      "metadata": {
        "id": "aNqkQe9EH2Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PollutantsPicc.NO2.mode())\n",
        "print(df.Volume.mode())"
      ],
      "metadata": {
        "id": "YCp0jxo5fbBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PollutantsShar.NO2.mode())\n",
        "print(df2.Volume.mode())"
      ],
      "metadata": {
        "id": "AuA8gCsae5lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 1, figsize=(10,4), sharex=True)\n",
        "sns.boxplot(y=PollutantsPicc.site, x=PollutantsPicc.temp, ax=axes[0],palette=\"Set2\")\n",
        "sns.boxplot(y=PollutantsShar.site, x=PollutantsShar.temp, ax=axes[1])\n",
        "axes[0].set_xlabel('')\n",
        "axes[0].set_ylabel('Piccadilly')\n",
        "axes[1].set_ylabel('Sharston')\n",
        "# Hide X and Y axes label marks\n",
        "axes[0].yaxis.set_tick_params(labelleft=False)\n",
        "axes[1].yaxis.set_tick_params(labelleft=False)\n",
        "\n",
        "# Hide X and Y axes tick marks\n",
        "axes[0].set_yticks([])\n",
        "axes[1].set_yticks([])\n",
        "axes[1].set_xlabel('Temperature (Celcius)')\n",
        "axes[0].label_outer()\n",
        "axes[1].label_outer()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUO1bfMTmE__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 1, figsize=(10,4), sharex=True)\n",
        "sns.boxplot(y=PollutantsPicc.site, x=PollutantsPicc.ws, ax=axes[0],palette=\"Set2\")\n",
        "sns.boxplot(y=PollutantsShar.site, x=PollutantsShar.ws, ax=axes[1])\n",
        "axes[0].set_xlabel('')\n",
        "axes[1].set_xlabel('wind speed (m/s)')\n",
        "axes[0].set_ylabel('Piccadilly')\n",
        "axes[1].set_ylabel('Sharston')\n",
        "# Hide X and Y axes label marks\n",
        "axes[0].yaxis.set_tick_params(labelleft=False)\n",
        "axes[1].yaxis.set_tick_params(labelleft=False)\n",
        "\n",
        "# Hide X and Y axes tick marks\n",
        "axes[0].set_yticks([])\n",
        "axes[1].set_yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JbIZK6w0nxTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 1, figsize=(10,8), sharex=True)\n",
        "sns.boxplot(y=PollutantsPicc.site, x=PollutantsPicc.NO2, ax=axes[0],palette=\"Set2\")\n",
        "sns.boxplot(y=PollutantsShar.site, x=PollutantsShar.NO2, ax=axes[1])\n",
        "axes[0].set_xlabel('')\n",
        "axes[0].set_ylabel('Piccadilly')\n",
        "axes[1].set_ylabel('Sharston')\n",
        "# Hide X and Y axes label marks\n",
        "axes[0].yaxis.set_tick_params(labelleft=False)\n",
        "axes[1].yaxis.set_tick_params(labelleft=False)\n",
        "\n",
        "# Hide X and Y axes tick marks\n",
        "axes[0].set_yticks([])\n",
        "axes[1].set_yticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VvrR0O-Jn2Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(7, 2, figsize=(15,21))\n",
        "\n",
        "sns.lineplot(x='temp', y='NO2', data=PollutantsPicc, ax=axes[0,0], label='Piccadilly')\n",
        "sns.lineplot(x='temp', y='NO2', data=PollutantsShar, ax=axes[0,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='ws', y='NO2', data=PollutantsPicc, ax=axes[1,0], label='Piccadilly')\n",
        "sns.lineplot(x='ws', y='NO2', data=PollutantsShar, ax=axes[1,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='date', y='NO2', data=PollutantsPicc20mn6dy4, ax=axes[2,0], label='Piccadilly')\n",
        "sns.lineplot(x='date', y='NO2', data=PollutantsShar20mn6dy4, ax=axes[2,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Year', y='NO2', data=PollutantsPicc, ax=axes[3,0], label='Piccadilly')\n",
        "sns.lineplot(x='Year', y='NO2', data=PollutantsShar, ax=axes[3,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='date', y='NO2', data=PollutantsPicc20mn6, ax=axes[4,0], label='Piccadilly')\n",
        "sns.lineplot(x='date', y='NO2', data=PollutantsShar20mn6, ax=axes[4,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='date', y='NO2', data=PollutantsPicc20mn6w23, ax=axes[5,0], label='Piccadilly')\n",
        "sns.lineplot(x='date', y='NO2', data=PollutantsShar20mn6w23, ax=axes[5,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='date', y='NO2_moving_avg4', data=PollutantsPicc20, ax=axes[6,0], label='Piccadilly')\n",
        "sns.lineplot(x='date', y='NO2_moving_avg4', data=PollutantsShar20, ax=axes[6,0], label='Sharston')\n",
        "\n",
        "\n",
        "sns.lineplot(x='temp', y='Volume', data=df, ax=axes[0,1], label='Piccadilly')\n",
        "sns.lineplot(x='temp', y='Volume', data=df2, ax=axes[0,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='ws', y='Volume', data=df, ax=axes[1,1], label='Piccadilly')\n",
        "sns.lineplot(x='ws', y='Volume', data=df2, ax=axes[1,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='date', y='Volume', data=df20mn6dy4, ax=axes[2,1], label='Piccadilly')\n",
        "sns.lineplot(x='date', y='Volume', data=df220mn6dy4, ax=axes[2,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Year', y='Volume', data=df, ax=axes[3,1], label='Piccadilly')\n",
        "sns.lineplot(x='Year', y='Volume', data=df2, ax=axes[3,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='date', y='Volume', data=df20mn6, ax=axes[4,1], label='Piccadilly')\n",
        "sns.lineplot(x='date', y='Volume', data=df220mn6, ax=axes[4,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='date', y='Volume', data=df20mn6w23, ax=axes[5,1], label='Piccadilly')\n",
        "sns.lineplot(x='date', y='Volume', data=df220mn6w23, ax=axes[5,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='date', y=df.Volume.rolling(window=100).mean(), data=df20, ax=axes[6,1], label='Piccadilly')\n",
        "sns.lineplot(x='date', y=df2.Volume.rolling(window=100).mean(), data=df220, ax=axes[6,1], label='Sharston')\n",
        "\n",
        "\n",
        "axes[4,0].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
        "axes[4,1].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
        "\n",
        "date_form = DateFormatter(\"%H\")\n",
        "axes[2,0].xaxis.set_major_formatter(date_form)\n",
        "axes[2,1].xaxis.set_major_formatter(date_form)\n",
        "date_form = DateFormatter(\"%d\")\n",
        "axes[5,0].xaxis.set_major_formatter(date_form)\n",
        "axes[5,1].xaxis.set_major_formatter(date_form)\n",
        "\n",
        "\n",
        "axes[0,0].legend(loc='upper right')\n",
        "axes[1,0].legend(loc='upper right')\n",
        "axes[2,0].legend(loc='upper right')\n",
        "axes[3,0].legend(loc='upper right')\n",
        "axes[4,0].legend(loc='upper right')\n",
        "axes[5,0].legend(loc='upper right')\n",
        "axes[6,0].legend(loc='upper right')\n",
        "\n",
        "axes[0,1].legend(loc='upper right')\n",
        "axes[1,1].legend(loc='upper right')\n",
        "axes[2,1].legend(loc='upper right')\n",
        "axes[3,1].legend(loc='upper right')\n",
        "axes[4,1].legend(loc='upper right')\n",
        "axes[5,1].legend(loc='upper right')\n",
        "axes[6,1].legend(loc='upper right')\n",
        "\n",
        "axes[0,0].set_xlabel('Temperature (Celcius)')\n",
        "axes[0,1].set_xlabel('Temperature (Celcius)')\n",
        "\n",
        "axes[1,0].set_xlabel('Wind Speed (m/s)')\n",
        "axes[1,1].set_xlabel('Wind Speed (m/s)')\n",
        "\n",
        "axes[2,0].set_xlabel('Hour')\n",
        "axes[2,0].set_title('Hourly NO2 concentrations on 04/06/2020',loc = 'center')\n",
        "axes[2,1].set_xlabel('Hour')\n",
        "axes[2,1].set_title('Hourly Traffic Volume on 04/06/2020',loc = 'center')\n",
        "axes[2,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[3,0].set_title('Yearly Aggregate NO2 concentrations',loc = 'center')\n",
        "axes[3,1].set_title('Yearly Aggregate Tarffic Volume',loc = 'center')\n",
        "axes[3,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[4,0].set_xlabel('Day')\n",
        "axes[4,0].set_title('NO2 concentrations in the month of June in 2020',loc = 'center')\n",
        "axes[4,1].set_xlabel('Day')\n",
        "axes[4,1].set_title('Traffic Volume in the month of June in 2020',loc = 'center')\n",
        "axes[4,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[5,0].set_xlabel('Day of Week')\n",
        "axes[5,0].set_title('NO2 concentrations on the first week of June in 2020',loc = 'center')\n",
        "axes[5,1].set_xlabel('Day of Week')\n",
        "axes[5,1].set_title('Traffic Volume on the first week of June in 2020',loc = 'center')\n",
        "axes[5,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[6,0].set_xlabel('Month')\n",
        "axes[6,0].set_ylabel('Rolling Mean NO2 (100)')\n",
        "axes[6,0].set_title('Rolling Mean NO2 (100) concentrations in 2020',loc = 'center')\n",
        "axes[6,1].set_xlabel('Month')\n",
        "axes[6,1].set_ylabel('Rolling Mean Volume (100)')\n",
        "axes[6,1].set_title('Rolling Mean Volume (100) in 2020',loc = 'center')\n",
        "axes[6,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.6)\n",
        "plt.show()\n",
        "#rolling means for better look?"
      ],
      "metadata": {
        "id": "sZUIV9NAH3gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(7, 2, figsize=(15,21))\n",
        "#fig.suptitle('NO2                             Volume')\n",
        "\n",
        "#First Column: NO2 ----\n",
        "\n",
        "#sns.lineplot(x='temp', y='NO2', data=PollutantsPicc, ax=axes[0,0], label='Piccadilly')\n",
        "#sns.lineplot(x='temp', y='NO2', data=PollutantsShar, ax=axes[0,0], label='Sharston')\n",
        "\n",
        "#sns.lineplot(x='ws', y='NO2', data=PollutantsPicc, ax=axes[1,0], label='Piccadilly')\n",
        "#sns.lineplot(x='ws', y='NO2', data=PollutantsShar, ax=axes[1,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Hour', y='NO2', data=PollutantsPicc, ax=axes[2,0], label='Piccadilly')\n",
        "sns.lineplot(x='Hour', y='NO2', data=PollutantsShar, ax=axes[2,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Year', y='NO2', data=PollutantsPicc, ax=axes[3,0], label='Piccadilly')\n",
        "sns.lineplot(x='Year', y='NO2', data=PollutantsShar, ax=axes[3,0], label='Sharston')\n",
        "\n",
        "#sns.lineplot(x='Day', y='NO2', data=PollutantsPicc, ax=axes[4,0], label='Piccadilly')\n",
        "#sns.lineplot(x='Day', y='NO2', data=PollutantsShar, ax=axes[4,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='DayofWeek', y='NO2', data=PollutantsPicc, ax=axes[4,0], label='Piccadilly')\n",
        "sns.lineplot(x='DayofWeek', y='NO2', data=PollutantsShar, ax=axes[4,0], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Month', y='NO2', data=PollutantsPicc, ax=axes[5,0], label='Piccadilly')\n",
        "sns.lineplot(x='Month', y='NO2', data=PollutantsShar, ax=axes[5,0], label='Sharston')\n",
        "\n",
        "\n",
        "#Second Column: Volume ----\n",
        "\n",
        "#sns.lineplot(x='temp', y='Volume', data=df, ax=axes[0,1], label='Piccadilly')\n",
        "#sns.lineplot(x='temp', y='Volume', data=df2, ax=axes[0,1], label='Sharston')\n",
        "\n",
        "#sns.lineplot(x='ws', y='Volume', data=df, ax=axes[1,1], label='Piccadilly')\n",
        "#sns.lineplot(x='ws', y='Volume', data=df2, ax=axes[1,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Hour', y='Volume', data=df, ax=axes[2,1], label='Piccadilly')\n",
        "sns.lineplot(x='Hour', y='Volume', data=df2, ax=axes[2,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Year', y='Volume', data=df, ax=axes[3,1], label='Piccadilly')\n",
        "sns.lineplot(x='Year', y='Volume', data=df2, ax=axes[3,1], label='Sharston')\n",
        "\n",
        "#sns.lineplot(x='Day', y='Volume', data=df, ax=axes[4,1], label='Piccadilly')\n",
        "#sns.lineplot(x='Day', y='Volume', data=df2, ax=axes[4,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='DayofWeek', y='Volume', data=df, ax=axes[4,1], label='Piccadilly')\n",
        "sns.lineplot(x='DayofWeek', y='Volume', data=df2, ax=axes[4,1], label='Sharston')\n",
        "\n",
        "sns.lineplot(x='Month', y='Volume', data=df, ax=axes[5,1], label='Piccadilly')\n",
        "sns.lineplot(x='Month', y='Volume', data=df2, ax=axes[5,1], label='Sharston')\n",
        "\n",
        "#Axes configuration ----\n",
        "\n",
        "#date_form = DateFormatter(\"%m-%Y\")\n",
        "#axes[0,0].xaxis.set_major_locator(MonthLocator(bymonth=(1,7)))\n",
        "#axes[0,0].xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "#axes[0,0].xaxis.set_major_formatter(date_form)\n",
        "#axes[0,1].xaxis.set_major_locator(MonthLocator(bymonth=(1,7)))\n",
        "#axes[0,1].xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "#axes[0,1].xaxis.set_major_formatter(date_form)\n",
        "#axes[4,0].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
        "#axes[4,1].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
        "\n",
        "#date_form = DateFormatter(\"%H\")\n",
        "#axes[2,0].xaxis.set_major_formatter(date_form)\n",
        "#axes[2,1].xaxis.set_major_formatter(date_form)\n",
        "#ate_form = DateFormatter(\"%d\")\n",
        "#axes[5,0].xaxis.set_major_formatter(date_form)\n",
        "#axes[5,1].xaxis.set_major_formatter(date_form)\n",
        "# labels\n",
        "#axes[0,0].set_xlim(['07-2016','07-2017'])\n",
        "\n",
        "axes[0,0].legend(loc='upper right')\n",
        "axes[1,0].legend(loc='upper right')\n",
        "axes[2,0].legend(loc='upper right')\n",
        "axes[3,0].legend(loc='upper right')\n",
        "axes[4,0].legend(loc='upper right')\n",
        "axes[5,0].legend(loc='upper right')\n",
        "axes[6,0].legend(loc='upper right')\n",
        "\n",
        "axes[0,1].legend(loc='upper right')\n",
        "axes[1,1].legend(loc='upper right')\n",
        "axes[2,1].legend(loc='upper right')\n",
        "axes[3,1].legend(loc='upper right')\n",
        "axes[4,1].legend(loc='upper right')\n",
        "axes[5,1].legend(loc='upper right')\n",
        "axes[6,1].legend(loc='upper right')\n",
        "\n",
        "axes[0,0].set_xlabel('Temperature (Celcius)')\n",
        "axes[0,1].set_xlabel('Temperature (Celcius)')\n",
        "\n",
        "axes[1,0].set_xlabel('Wind Speed (m/s)')\n",
        "axes[1,1].set_xlabel('Wind Speed (m/s)')\n",
        "\n",
        "axes[2,0].set_xlabel('Hour')\n",
        "axes[2,0].set_title('Hourly NO2 concentrations',loc = 'center')\n",
        "axes[2,1].set_xlabel('Hour')\n",
        "axes[2,1].set_title('Hourly Traffic Volume',loc = 'center')\n",
        "axes[2,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[3,0].set_title('Yearly NO2 concentrations',loc = 'center')\n",
        "axes[3,1].set_title('Yearly Tarffic Volume',loc = 'center')\n",
        "axes[3,1].set_ylabel('Traffic Volume')\n",
        "axes[3,1].set_xticks([2016,2017,2018,2019,2020,2021])\n",
        "\n",
        "#axes[4,0].set_xlabel('Day')\n",
        "#axes[4,0].set_title('NO2 concentrations Daily throughout a month',loc = 'center')\n",
        "#axes[4,1].set_xlabel('Day')\n",
        "#axes[4,1].set_title('Traffic Volume Daily throughout a month',loc = 'center')\n",
        "#axes[4,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[4,0].set_xlabel('Day of Week')\n",
        "axes[4,0].set_title('NO2 concentrations Daily throughout a week',loc = 'center')\n",
        "axes[4,0].set_xticks([0,1,2,3,4,5,6])\n",
        "axes[4,0].set_xticklabels(['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\n",
        "axes[4,1].set_xlabel('Day of Week')\n",
        "axes[4,1].set_title('Traffic Volume Daily throughout a week',loc = 'center')\n",
        "axes[4,1].set_ylabel('Traffic Volume')\n",
        "axes[4,1].set_xticks([0,1,2,3,4,5,6])\n",
        "axes[4,1].set_xticklabels(['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\n",
        "\n",
        "axes[5,0].set_xlabel('Month')\n",
        "axes[5,0].set_title('Monthly NO2 concentrations',loc = 'center')\n",
        "axes[5,0].set_xticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "axes[5,0].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
        "axes[5,1].set_xlabel('Month')\n",
        "axes[5,1].set_title('Monthly Traffic Volume',loc = 'center')\n",
        "axes[5,1].set_ylabel('Traffic Volume')\n",
        "axes[5,1].set_xticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "axes[5,1].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
        "\n",
        "\n",
        "#axes[0,0].set_yscale('log')\n",
        "#axes[0,1].set_yscale('log')\n",
        "\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.6)\n",
        "plt.show()\n",
        "#rolling means for better look?"
      ],
      "metadata": {
        "id": "wpP-WEIgy11v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(7, 2, figsize=(15,21))\n",
        "#fig.suptitle('NO2                             Volume')\n",
        "\n",
        "#First Column: NO2 ----\n",
        "\n",
        "#sns.lineplot(x='temp', y='NO2', data=PollutantsPicc, ax=axes[0,0], label='Picaddily')\n",
        "#sns.lineplot(x='temp', y='NO2', data=PollutantsShar, ax=axes[0,0], label='Sharston')\n",
        "\n",
        "#sns.lineplot(x='ws', y='NO2', data=PollutantsPicc, ax=axes[1,0], label='Picaddily')\n",
        "#sns.lineplot(x='ws', y='NO2', data=PollutantsShar, ax=axes[1,0], label='Sharston')\n",
        "\n",
        "sns.boxplot(x='Hour', y='NO2', data=PollutantsPicc, ax=axes[2,0])\n",
        "\n",
        "sns.boxplot(x='Year', y='NO2', data=PollutantsPicc, ax=axes[3,0])\n",
        "\n",
        "sns.boxplot(x='Day', y='NO2', data=PollutantsPicc, ax=axes[4,0])\n",
        "\n",
        "sns.boxplot(x='DayofWeek', y='NO2', data=PollutantsPicc, ax=axes[5,0])\n",
        "\n",
        "sns.boxplot(x='Month', y='NO2', data=PollutantsPicc, ax=axes[6,0])\n",
        "\n",
        "\n",
        "#Second Column: Volume ----\n",
        "\n",
        "#sns.lineplot(x='temp', y='Volume', data=df, ax=axes[0,1], label='Picaddily')\n",
        "#sns.lineplot(x='temp', y='Volume', data=df2, ax=axes[0,1], label='Sharston')\n",
        "\n",
        "#sns.lineplot(x='ws', y='Volume', data=df, ax=axes[1,1], label='Picaddily')\n",
        "#sns.lineplot(x='ws', y='Volume', data=df2, ax=axes[1,1], label='Sharston')\n",
        "\n",
        "sns.boxplot(x='Hour', y='Volume', data=df, ax=axes[2,1])\n",
        "\n",
        "sns.boxplot(x='Year', y='Volume', data=df, ax=axes[3,1])\n",
        "\n",
        "sns.boxplot(x='Day', y='Volume', data=df, ax=axes[4,1])\n",
        "\n",
        "sns.boxplot(x='DayofWeek', y='Volume', data=df, ax=axes[5,1])\n",
        "\n",
        "sns.boxplot(x='Month', y='Volume', data=df, ax=axes[6,1])\n",
        "\n",
        "#Axes configuration ----\n",
        "\n",
        "#date_form = DateFormatter(\"%m-%Y\")\n",
        "#axes[0,0].xaxis.set_major_locator(MonthLocator(bymonth=(1,7)))\n",
        "#axes[0,0].xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "#axes[0,0].xaxis.set_major_formatter(date_form)\n",
        "#axes[0,1].xaxis.set_major_locator(MonthLocator(bymonth=(1,7)))\n",
        "#axes[0,1].xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "#axes[0,1].xaxis.set_major_formatter(date_form)\n",
        "#axes[4,0].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
        "#axes[4,1].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
        "\n",
        "#date_form = DateFormatter(\"%H\")\n",
        "#axes[2,0].xaxis.set_major_formatter(date_form)\n",
        "#axes[2,1].xaxis.set_major_formatter(date_form)\n",
        "#ate_form = DateFormatter(\"%d\")\n",
        "#axes[5,0].xaxis.set_major_formatter(date_form)\n",
        "#axes[5,1].xaxis.set_major_formatter(date_form)\n",
        "# labels\n",
        "#axes[0,0].set_xlim(['07-2016','07-2017'])\n",
        "\n",
        "axes[0,0].legend(loc='upper right')\n",
        "axes[1,0].legend(loc='upper right')\n",
        "axes[2,0].legend(loc='upper right')\n",
        "axes[3,0].legend(loc='upper right')\n",
        "axes[4,0].legend(loc='upper right')\n",
        "axes[5,0].legend(loc='upper right')\n",
        "axes[6,0].legend(loc='upper right')\n",
        "\n",
        "axes[0,1].legend(loc='upper right')\n",
        "axes[1,1].legend(loc='upper right')\n",
        "axes[2,1].legend(loc='upper right')\n",
        "axes[3,1].legend(loc='upper right')\n",
        "axes[4,1].legend(loc='upper right')\n",
        "axes[5,1].legend(loc='upper right')\n",
        "axes[6,1].legend(loc='upper right')\n",
        "\n",
        "axes[0,0].set_xlabel('Temperature (Celcius)')\n",
        "axes[0,1].set_xlabel('Temperature (Celcius)')\n",
        "\n",
        "axes[1,0].set_xlabel('Wind Speed (m/s)')\n",
        "axes[1,1].set_xlabel('Wind Speed (m/s)')\n",
        "\n",
        "axes[2,0].set_xlabel('Hour')\n",
        "axes[2,0].set_title('Hourly NO2 concentrations (Piccadilly)',loc = 'center')\n",
        "axes[2,1].set_xlabel('Hour')\n",
        "axes[2,1].set_title('Hourly Traffic Volume (Piccadilly)',loc = 'center')\n",
        "axes[2,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[3,0].set_title('Yearly NO2 concentrations (Piccadilly)',loc = 'center')\n",
        "axes[3,1].set_title('Yearly Tarffic Volume (Piccadilly)',loc = 'center')\n",
        "axes[3,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[4,0].set_xlabel('Day')\n",
        "axes[4,0].set_title('NO2 concentrations Daily throughout a month (Piccadilly)',loc = 'center')\n",
        "axes[4,1].set_xlabel('Day')\n",
        "axes[4,1].set_title('Traffic Volume Daily throughout a month (Piccadilly)',loc = 'center')\n",
        "axes[4,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[5,0].set_xlabel('Day of Week')\n",
        "axes[5,0].set_title('NO2 concentrations Daily throughout a week (Piccadilly)',loc = 'center')\n",
        "axes[5,0].set_xticks([0,1,2,3,4,5,6])\n",
        "axes[5,0].set_xticklabels(['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\n",
        "axes[5,1].set_xlabel('Day of Week')\n",
        "axes[5,1].set_title('Traffic Volume Daily throughout a week (Piccadilly)',loc = 'center')\n",
        "axes[5,1].set_ylabel('Traffic Volume')\n",
        "axes[5,1].set_xticks([0,1,2,3,4,5,6])\n",
        "axes[5,1].set_xticklabels(['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\n",
        "\n",
        "axes[6,0].set_xlabel('Month')\n",
        "axes[6,0].set_title('Monthly NO2 concentrations (Piccadilly)',loc = 'center')\n",
        "axes[6,0].set_xticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "axes[6,0].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
        "axes[6,1].set_xlabel('Month')\n",
        "axes[6,1].set_title('Monthly Traffic Volume (Piccadilly)',loc = 'center')\n",
        "axes[6,1].set_ylabel('Traffic Volume')\n",
        "axes[6,1].set_xticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "axes[6,1].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
        "\n",
        "\n",
        "#axes[0,0].set_yscale('log')\n",
        "#axes[0,1].set_yscale('log')\n",
        "\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.6)\n",
        "plt.show()\n",
        "#rolling means for better look?"
      ],
      "metadata": {
        "id": "p82n4NRU1xYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(7, 2, figsize=(15,21))\n",
        "#fig.suptitle('NO2                             Volume')\n",
        "\n",
        "#First Column: NO2 ----\n",
        "\n",
        "#sns.lineplot(x='temp', y='NO2', data=PollutantsPicc, ax=axes[0,0], label='Picaddily')\n",
        "#sns.lineplot(x='temp', y='NO2', data=PollutantsShar, ax=axes[0,0], label='Sharston')\n",
        "\n",
        "#sns.lineplot(x='ws', y='NO2', data=PollutantsPicc, ax=axes[1,0], label='Picaddily')\n",
        "#sns.lineplot(x='ws', y='NO2', data=PollutantsShar, ax=axes[1,0], label='Sharston')\n",
        "\n",
        "sns.boxplot(x='Hour', y='NO2', data=PollutantsShar, ax=axes[2,0])\n",
        "\n",
        "sns.boxplot(x='Year', y='NO2', data=PollutantsShar, ax=axes[3,0])\n",
        "\n",
        "sns.boxplot(x='Day', y='NO2', data=PollutantsShar, ax=axes[4,0])\n",
        "\n",
        "sns.boxplot(x='DayofWeek', y='NO2', data=PollutantsShar, ax=axes[5,0])\n",
        "\n",
        "sns.boxplot(x='Month', y='NO2', data=PollutantsShar, ax=axes[6,0])\n",
        "\n",
        "\n",
        "#Second Column: Volume ----\n",
        "\n",
        "#sns.lineplot(x='temp', y='Volume', data=df, ax=axes[0,1], label='Picaddily')\n",
        "#sns.lineplot(x='temp', y='Volume', data=df2, ax=axes[0,1], label='Sharston')\n",
        "\n",
        "#sns.lineplot(x='ws', y='Volume', data=df, ax=axes[1,1], label='Picaddily')\n",
        "#sns.lineplot(x='ws', y='Volume', data=df2, ax=axes[1,1], label='Sharston')\n",
        "\n",
        "sns.boxplot(x='Hour', y='Volume', data=df2, ax=axes[2,1])\n",
        "\n",
        "sns.boxplot(x='Year', y='Volume', data=df2, ax=axes[3,1])\n",
        "\n",
        "sns.boxplot(x='Day', y='Volume', data=df2, ax=axes[4,1])\n",
        "\n",
        "sns.boxplot(x='DayofWeek', y='Volume', data=df2, ax=axes[5,1])\n",
        "\n",
        "sns.boxplot(x='Month', y='Volume', data=df2, ax=axes[6,1])\n",
        "\n",
        "#Axes configuration ----\n",
        "\n",
        "#date_form = DateFormatter(\"%m-%Y\")\n",
        "#axes[0,0].xaxis.set_major_locator(MonthLocator(bymonth=(1,7)))\n",
        "#axes[0,0].xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "#axes[0,0].xaxis.set_major_formatter(date_form)\n",
        "#axes[0,1].xaxis.set_major_locator(MonthLocator(bymonth=(1,7)))\n",
        "#axes[0,1].xaxis.set_major_locator(plt.MaxNLocator(6))\n",
        "#axes[0,1].xaxis.set_major_formatter(date_form)\n",
        "#axes[4,0].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
        "#axes[4,1].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
        "\n",
        "#date_form = DateFormatter(\"%H\")\n",
        "#axes[2,0].xaxis.set_major_formatter(date_form)\n",
        "#axes[2,1].xaxis.set_major_formatter(date_form)\n",
        "#ate_form = DateFormatter(\"%d\")\n",
        "#axes[5,0].xaxis.set_major_formatter(date_form)\n",
        "#axes[5,1].xaxis.set_major_formatter(date_form)\n",
        "# labels\n",
        "#axes[0,0].set_xlim(['07-2016','07-2017'])\n",
        "\n",
        "axes[0,0].legend(loc='upper right')\n",
        "axes[1,0].legend(loc='upper right')\n",
        "axes[2,0].legend(loc='upper right')\n",
        "axes[3,0].legend(loc='upper right')\n",
        "axes[4,0].legend(loc='upper right')\n",
        "axes[5,0].legend(loc='upper right')\n",
        "axes[6,0].legend(loc='upper right')\n",
        "\n",
        "axes[0,1].legend(loc='upper right')\n",
        "axes[1,1].legend(loc='upper right')\n",
        "axes[2,1].legend(loc='upper right')\n",
        "axes[3,1].legend(loc='upper right')\n",
        "axes[4,1].legend(loc='upper right')\n",
        "axes[5,1].legend(loc='upper right')\n",
        "axes[6,1].legend(loc='upper right')\n",
        "\n",
        "axes[0,0].set_xlabel('Temperature (Celcius)')\n",
        "axes[0,1].set_xlabel('Temperature (Celcius)')\n",
        "\n",
        "axes[1,0].set_xlabel('Wind Speed (m/s)')\n",
        "axes[1,1].set_xlabel('Wind Speed (m/s)')\n",
        "\n",
        "axes[2,0].set_xlabel('Hour')\n",
        "axes[2,0].set_title('Hourly NO2 concentrations (Sharston)',loc = 'center')\n",
        "axes[2,1].set_xlabel('Hour')\n",
        "axes[2,1].set_title('Hourly Traffic Volume (Sharston)',loc = 'center')\n",
        "axes[2,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[3,0].set_title('Yearly NO2 concentrations (Sharston)',loc = 'center')\n",
        "axes[3,1].set_title('Yearly Tarffic Volume (Sharston)',loc = 'center')\n",
        "axes[3,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[4,0].set_xlabel('Day')\n",
        "axes[4,0].set_title('NO2 concentrations Daily throughout a month (Sharston)',loc = 'center')\n",
        "axes[4,1].set_xlabel('Day')\n",
        "axes[4,1].set_title('Traffic Volume Daily throughout a month (Sharston)',loc = 'center')\n",
        "axes[4,1].set_ylabel('Traffic Volume')\n",
        "\n",
        "axes[5,0].set_xlabel('Day of Week')\n",
        "axes[5,0].set_title('NO2 concentrations Daily throughout a week (Sharston)',loc = 'center')\n",
        "axes[5,0].set_xticks([0,1,2,3,4,5,6])\n",
        "axes[5,0].set_xticklabels(['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\n",
        "axes[5,1].set_xlabel('Day of Week')\n",
        "axes[5,1].set_title('Traffic Volume Daily throughout a week (Sharston)',loc = 'center')\n",
        "axes[5,1].set_ylabel('Traffic Volume')\n",
        "axes[5,1].set_xticks([0,1,2,3,4,5,6])\n",
        "axes[5,1].set_xticklabels(['Mon','Tue','Wed','Thu','Fri','Sat','Sun'])\n",
        "\n",
        "axes[6,0].set_xlabel('Month')\n",
        "axes[6,0].set_title('Monthly NO2 concentrations (Sharston)',loc = 'center')\n",
        "axes[6,0].set_xticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "axes[6,0].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
        "axes[6,1].set_xlabel('Month')\n",
        "axes[6,1].set_title('Monthly Traffic Volume (Sharston)',loc = 'center')\n",
        "axes[6,1].set_ylabel('Traffic Volume')\n",
        "axes[6,1].set_xticks([1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "axes[6,1].set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
        "\n",
        "\n",
        "\n",
        "#axes[0,0].set_yscale('log')\n",
        "#axes[0,1].set_yscale('log')\n",
        "\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.6)\n",
        "plt.show()\n",
        "#rolling means for better look?"
      ],
      "metadata": {
        "id": "JyZ4t3AN5KOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics import tsaplots"
      ],
      "metadata": {
        "id": "rmJRXPYcmlcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsPicc.NO2, lags=60, color='g',  title='Autocorrelation function (Piccadilly)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w9IpyErTmSeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsPicc.NO2, lags=4000, color='g',  title='Autocorrelation function (Piccadilly)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "75XcM-XUNTIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsPicc.NO2, lags=40000, color='g', title='Autocorrelation function (Piccadilly)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BhIVNO4woMgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsShar.NO2, lags=60, color='g', title='Autocorrelation function (Sharston)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qSGFjOGImavP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsShar.NO2, lags=4000, color='g', title='Autocorrelation function (Sharston)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H54I2cOdNI3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot autocorrelation function\n",
        "fig= plt.figure(figsize=(10,5))\n",
        "tsaplots.plot_acf(PollutantsShar.NO2, lags=40000, color='g', title='Autocorrelation function (Sharston)')\n",
        "plt.xlabel('lags')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xN04IspMoTmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig= plt.figure(figsize=(12,8))\n",
        "sns.boxplot(PollutantsPicc.wd, PollutantsPicc.NO2)\n",
        "plt.xlabel('Wind Direction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D096pZjN33YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig= plt.figure(figsize=(12,8))\n",
        "sns.boxplot(PollutantsShar.wd, PollutantsShar.NO2)\n",
        "plt.xlabel('Wind Direction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tDovT1-H6eNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Piccadilly site Modeling**"
      ],
      "metadata": {
        "id": "3BWdbgujbiku"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8fTYm_p9PF"
      },
      "source": [
        "## **Prepare Data Where X contains the predictor variables and y the target**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb1"
      ],
      "metadata": {
        "id": "cy2KqVvbTXzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsPicc.iloc[:, [5,6,7]].values\n",
        "y = PollutantsPicc.iloc[:,3].values"
      ],
      "metadata": {
        "id": "C6-XO7SS3RBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = PollutantsPicc.iloc[:, [6,7,14,15,16,17,18,19,20,21]].values\n",
        "y = PollutantsPicc.iloc[:,3].values"
      ],
      "metadata": {
        "id": "VH-7yh6yM-o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb2"
      ],
      "metadata": {
        "id": "Pgcq1Sh2TYlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsPicc.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsPicc.iloc[:,3].values"
      ],
      "metadata": {
        "id": "Q9hjejo43mMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = PollutantsPicc.iloc[:, [6,7,16,17,19,20,22,23,24,26,27,28,34,35,36,37,38,39,40,41]].values\n",
        "y = PollutantsPicc.iloc[:,3].values"
      ],
      "metadata": {
        "id": "vNmOKWgL6YFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb3"
      ],
      "metadata": {
        "id": "U_u66Qf1TZjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = df.iloc[:, [6,9,10,11,14,15,16,17,18,19,20]].values\n",
        "y = df.iloc[:,8].values"
      ],
      "metadata": {
        "id": "bRrJZAfx3tmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = df.iloc[:, [6,10,11,14,15,16,17,18,19,20,26,27,\n",
        "                28,29,30,31,32,33,34,35,36,37,38,39,40,41]].values\n",
        "y = df.iloc[:,8].values\n"
      ],
      "metadata": {
        "id": "1kcckEZdgM5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb4"
      ],
      "metadata": {
        "id": "m8DZdbuSTaQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsPicc.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsPicc.iloc[:,22].values"
      ],
      "metadata": {
        "id": "uZTByLb7PnZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = PollutantsPicc.iloc[:, [6,7,16,17,19,20,21,22,23,24,25,26,27,28,34,35,36,37,38,39,40,41]].values\n",
        "y = PollutantsPicc.iloc[:,29].values"
      ],
      "metadata": {
        "id": "TztBGggIR0X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb5"
      ],
      "metadata": {
        "id": "zYuAtNyGTfDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsPicc.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsPicc.iloc[:,23].values"
      ],
      "metadata": {
        "id": "S0xEIRg4JARp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = PollutantsPicc.iloc[:, [6,7,16,17,19,20,22,23,24,26,27,28,34,35,36,37,38,39,40,41]].values\n",
        "y = PollutantsPicc.iloc[:,30].values"
      ],
      "metadata": {
        "id": "hMtPQpJxR3u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb6"
      ],
      "metadata": {
        "id": "lOj0Bgm7TfwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsPicc.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsPicc.iloc[:,24].values"
      ],
      "metadata": {
        "id": "EKsQQ1TSJhYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = PollutantsPicc.iloc[:, [6,7,16,17,19,20,22,23,24,26,27,28,34,35,36,37,38,39,40,41]].values\n",
        "y = PollutantsPicc.iloc[:,31].values"
      ],
      "metadata": {
        "id": "HeNtN6nGBKHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb7"
      ],
      "metadata": {
        "id": "6Pm-RE4EThm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = df.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = df.iloc[:,23].values"
      ],
      "metadata": {
        "id": "L31nCOhfoLK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#onehotencoded\n",
        "X = df.iloc[:, [6,7,16,17,19,20,22,23,24,26,27,28,34,35,36,37,38,39,40,41]].values\n",
        "y = df.iloc[:,32].values"
      ],
      "metadata": {
        "id": "Kc95LhJdR7RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform Train Test Split"
      ],
      "metadata": {
        "id": "q7Rvxc0OXt_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmqV7E3e8gND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e28e4d9-3255-4906-ae62-6c4dcc9ff628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StandardScaler()\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "print(scaler.fit(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR6OWs9TloY2"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random Forest**"
      ],
      "metadata": {
        "id": "m3fXf1ZiUNtY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0yRoLfRthB0"
      },
      "outputs": [],
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcztH4QXtnaw"
      },
      "outputs": [],
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 10 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
        "                               n_iter = 10, cv = 10, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsCBbH4NPBL9"
      },
      "outputs": [],
      "source": [
        "#print the best parameters found\n",
        "rf_random.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run and fit the model\n",
        "rfregressor = RandomForestRegressor(n_estimators = 200,max_depth = 10, \n",
        "                                  max_features= 'sqrt', min_samples_split = 5, min_samples_leaf= 2,\n",
        "                                  bootstrap = True, random_state = 42)\n",
        "rfregressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "fiN0XtCq4obY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55BQBvE6PE1M"
      },
      "outputs": [],
      "source": [
        "#Run and fit the model\n",
        "rfregressor = RandomForestRegressor(n_estimators = 400,max_depth = 60, \n",
        "                                  max_features= 'sqrt', min_samples_split = 10, min_samples_leaf= 1,\n",
        "                                  bootstrap = False, random_state = 42)\n",
        "rfregressor.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8XbKzVR_F4p"
      },
      "outputs": [],
      "source": [
        "# Generates Predictions\n",
        "predictr=rfregressor.predict(X_test)\n",
        "predictr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Algorithm\n",
        "from sklearn import metrics\n",
        "print(rfregressor.score(X_train, y_train))\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictr))  \n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictr))  \n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, \n",
        "                                                                     predictr)))\n",
        "print(\"R2: %.4f\" % r2_score(y_test, predictr))"
      ],
      "metadata": {
        "id": "_SQQIXZ8UxVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CV score\n",
        "scores = cross_val_score(rfregressor, X_train, y_train, \n",
        "                         scoring='r2', cv=10,n_jobs=-1)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "p1O11Cr5unsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RF Plots "
      ],
      "metadata": {
        "id": "ikbjCk0ZAv6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generat a plot of observed vs predicted values on a log scale\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictr, c='crimson')\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "p1 = max(max(predictr), max(y_test))\n",
        "p2 = min(min(predictr), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MUnhil7akWW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generat a plot of observed vs predicted values\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictr, c='crimson')\n",
        "\n",
        "p1 = max(max(predictr), max(y_test))\n",
        "p2 = min(min(predictr), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LQYPuTCuTMJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **XGBoost**"
      ],
      "metadata": {
        "id": "OsaCbAOpUXbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set range for Number estimators\n",
        "n_estimators = [int(x) for x in np.linspace(start = 1000, stop = 10000, num = 10)]\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Learning rate investigated values\n",
        "learning_rate = [0.01,0.03,0.09,0.1,0.3]\n",
        "# colsample_by tree investigated values\n",
        "colsample_bytree = [0.3,0.7]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_depth': max_depth,\n",
        "               'learning_rate': learning_rate,\n",
        "               'colsample_bytree': colsample_bytree}\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "mnQjozJSlFQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "xgbr = xgb.XGBRegressor()\n",
        "# Random search of parameters, using 10 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "xgbr_random = RandomizedSearchCV(estimator = xgbr, param_distributions = random_grid, \n",
        "                               n_iter = 10, cv = 10, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "xgbr_random.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "j0qxnieSnThb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find best parameters from random search\n",
        "xgbr_random.best_params_"
      ],
      "metadata": {
        "id": "Z61e3wdJndKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "excNrGU_ZU4p"
      },
      "outputs": [],
      "source": [
        "#Run and fit the model\n",
        "xgbr = xgb.XGBRegressor(n_estimators=4000, max_depth=10,\n",
        "                       learning_rate=0.03,\n",
        "                       colsample_bytree = 0.3,\n",
        "                       random_state=42)\n",
        "xgbr.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run and fit the model\n",
        "xgbr = xgb.XGBRegressor(n_estimators=10000, max_depth=9,\n",
        "                       learning_rate=0.03,\n",
        "                       colsample_bytree = 0.7,\n",
        "                       random_state=42)\n",
        "xgbr.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "Aikg0mlqeHBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqx0BaXxoY8k"
      },
      "outputs": [],
      "source": [
        "# generate Predictions\n",
        "predictx=xgbr.predict(X_test)\n",
        "predictx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVr1d73MUlBJ"
      },
      "outputs": [],
      "source": [
        "# Evaluating the Algorithm\n",
        "from sklearn import metrics\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictx))  \n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictx))  \n",
        "print('Root Mean Squared Error:', \n",
        "      np.sqrt(metrics.mean_squared_error(y_test, predictx)))\n",
        "print(\"R2: %.4f\" % r2_score(y_test, predictx))\n",
        "print(xgbr.score(X_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CV score\n",
        "scores = cross_val_score(xgbr, X_train, y_train, \n",
        "                         scoring='r2', cv=10,n_jobs=-1)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "4-0SiVCf5ph-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGB Plots"
      ],
      "metadata": {
        "id": "1ibp2lf3RhcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generat a plot of observed vs predicted values on a log scale\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictx, c='crimson')\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "p1 = max(max(predictx), max(y_test))\n",
        "p2 = min(min(predictx), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zUzJ5Z4lkYCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a plot of observed vs predicted values\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictx, c='crimson')\n",
        "\n",
        "p1 = max(max(predictx), max(y_test))\n",
        "p2 = min(min(predictx), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kOg26HYvESsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sharston site Modeling**"
      ],
      "metadata": {
        "id": "H1xWLhBJbamu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data Where X contains the predictor variables and y the target"
      ],
      "metadata": {
        "id": "kZ7ZZzlakDZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb1"
      ],
      "metadata": {
        "id": "f-jB73Y4xB1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsShar.iloc[:, [5,6,7]].values\n",
        "y = PollutantsShar.iloc[:,3].values"
      ],
      "metadata": {
        "id": "xveglSlFKyJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,7,26,27,28,29,30,31,32]].values\n",
        "y = PollutantsShar.iloc[:,3].values"
      ],
      "metadata": {
        "id": "OWx4A8WkxK77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb2"
      ],
      "metadata": {
        "id": "-YBIWgtWxCWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsShar.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsShar.iloc[:,3].values"
      ],
      "metadata": {
        "id": "mWZHtjjBPFtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,7,14,15,16,17,18,19,20,26,27,28,29,30,31,32,33,34,35,36,37,38,39]].values\n",
        "y = PollutantsShar.iloc[:,3].values"
      ],
      "metadata": {
        "id": "avPhDu0EyWYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb3"
      ],
      "metadata": {
        "id": "Y935VfW_xDEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = df2.iloc[:, [6,9,10,11,14,15,16,17,18,19,20]].values\n",
        "y = df2.iloc[:,8].values"
      ],
      "metadata": {
        "id": "t1QyGvWZpukF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = df2.iloc[:, [6,10,11,14,15,16,17,18,19,20,21,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]].values\n",
        "y = df2.iloc[:,8].values"
      ],
      "metadata": {
        "id": "1oNlPp8H0sPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb4"
      ],
      "metadata": {
        "id": "qGRXrGy3xEAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsShar.iloc[:, [5,6,7,14,15,16,17,18,19,20,21]].values\n",
        "y = PollutantsShar.iloc[:,22].values"
      ],
      "metadata": {
        "id": "8ecVvgYqa57j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,7,14,15,16,17,18,19,20,21,26,27,28,29,30,31,32,33,34,35,36,37,38,39]].values\n",
        "y = PollutantsShar.iloc[:,22].values"
      ],
      "metadata": {
        "id": "soW4DS541I3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb5"
      ],
      "metadata": {
        "id": "b0R5cjjIxE1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsShar.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsShar.iloc[:,23].values\n",
        "#dependant variable is now the rolling mean(25)"
      ],
      "metadata": {
        "id": "hNV-le1ia3rE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac1f5a8b-92db-449d-f08e-40b0a33c3ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9890059805115405"
            ]
          },
          "metadata": {},
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,7,14,15,16,17,18,19,20,21,26,27,28,29,30,31,32,33,34,35,36,37,38,39]].values\n",
        "y = PollutantsShar.iloc[:,23].values"
      ],
      "metadata": {
        "id": "TyuD-hO31fwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb6"
      ],
      "metadata": {
        "id": "NeOz158ExFpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = PollutantsShar.iloc[:, [5,6,7,14,15,16,17,18,19,20]].values\n",
        "y = PollutantsShar.iloc[:,24].values"
      ],
      "metadata": {
        "id": "G4cxQnXOPVS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,7,14,15,16,17,18,19,20,21,26,27,28,29,30,31,32,33,34,35,36,37,38,39]].values\n",
        "y = PollutantsShar.iloc[:,24].values"
      ],
      "metadata": {
        "id": "xjH-yDlr10cA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Varcomb7"
      ],
      "metadata": {
        "id": "6F84nfMZxGYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#variable numbering depends on order of feature engineering and merging\n",
        "X = df2.iloc[:, [6,9,10,11,14,15,16,17,18,19,20]].values\n",
        "y = df2.iloc[:,23].values"
      ],
      "metadata": {
        "id": "R8mf9mAIO3Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE\n",
        "X = PollutantsShar.iloc[:, [6,10,11,14,15,16,17,18,19,20,21,26,27,28,29,30,31,32,33,34,35,36,37,38,39]].values\n",
        "y = PollutantsShar.iloc[:,23].values"
      ],
      "metadata": {
        "id": "srOACLLZtXW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform Train test split"
      ],
      "metadata": {
        "id": "v6trKZUfI9pD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4yjPwsA3uxN",
        "outputId": "fa723b4d-b328-4317-eedc-5ec5637474a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StandardScaler()\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "print(scaler.fit(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdxiZ4NaoGBe"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "AWsKhxP0NE6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Grid for Random Search\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "9ayhUZmkkhyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 10 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
        "                               n_iter = 10, cv = 10, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "6D-yfyzykiXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find best parameters\n",
        "rf_random.best_params_"
      ],
      "metadata": {
        "id": "f-ofxhiJklMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run and fit the model fro Varcomb1\n",
        "rfregressor = RandomForestRegressor(n_estimators = 200,max_depth = 10, \n",
        "                                  max_features= 'sqrt', min_samples_split = 5, min_samples_leaf= 2,\n",
        "                                  bootstrap = True, random_state = 42)\n",
        "rfregressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "aigSSVK6ZpIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdKsKsr03_jR"
      },
      "outputs": [],
      "source": [
        "#Run and fit the model for Varcomb2\n",
        "rfregressor = RandomForestRegressor(n_estimators = 400,max_depth = 60, \n",
        "                                  max_features= 'sqrt', min_samples_split = 10, min_samples_leaf= 1,\n",
        "                                  bootstrap = False, random_state = 42)\n",
        "rfregressor.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV1_OXqEoS1u"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "predictr=rfregressor.predict(X_test)\n",
        "predictr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmK7HTEwoqa7"
      },
      "outputs": [],
      "source": [
        "#Evaluation\n",
        "print(\"MSE: %.4f\" % mean_squared_error(y_test, predictr))\n",
        "print(\"RMSE: %.4f\" % math.sqrt(mean_squared_error(y_test, predictr)))\n",
        "print(\"MAE: %.4f\" % mean_absolute_error(y_test, predictr))\n",
        "print(\"R2: %.4f\" % r2_score(y_test, predictr))\n",
        "print(rfregressor.score(X_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CV score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(rfregressor, X_train, y_train, \n",
        "                         scoring='r2', cv=10,n_jobs=-1)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "FVvm32sbeOM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RF plots"
      ],
      "metadata": {
        "id": "sK3-zAWFF7gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scatter plot to show predicted and Observed values\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictr, c='crimson')\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "\n",
        "p1 = max(max(predictr), max(y_test))\n",
        "p2 = min(min(predictr), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y1nBWGx9kq3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vcomb1\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictr, c='crimson')\n",
        "\n",
        "\n",
        "p1 = max(max(predictr), max(y_test))\n",
        "p2 = min(min(predictr), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-eaaCs6KxUdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "l5UrwIbFNPky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Grid for Random Search\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 2000, stop = 20000, num = 10)]\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# \n",
        "learning_rate = [0.01,0.03,0.09,0.1,0.3]\n",
        "# \n",
        "colsample_bytree = [0.3,0.7]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_depth': max_depth,\n",
        "               'learning_rate': learning_rate,\n",
        "               'colsample_bytree': colsample_bytree}\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "OhO5eYginyJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "xgb = xgb.XGBRegressor()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = random_grid, \n",
        "                               n_iter = 10, cv = 5, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "xgb_random.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "5GVeysstnymm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_random.best_params_"
      ],
      "metadata": {
        "id": "Khca8n_1n76b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vcomb1\n",
        "xgbr = xgb.XGBRegressor(n_estimators=4000, max_depth=10,\n",
        "                       learning_rate=0.03,\n",
        "                       colsample_bytree = 0.3,\n",
        "                       random_state=42)\n",
        "xgbr.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "9V9evlIa4LV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgbr = xgb.XGBRegressor(n_estimators=10000, max_depth=9,\n",
        "                       learning_rate=0.03,\n",
        "                       colsample_bytree = 0.7,\n",
        "                       random_state=42)\n",
        "xgbr.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "DN_yfjywNT6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuPbwpQgp4k-"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "predictx=xgbr.predict(X_test)\n",
        "predictx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZCb6HCap4mB"
      },
      "outputs": [],
      "source": [
        "print(\"MSE: %.4f\" % mean_squared_error(y_test, predictx))\n",
        "print(\"RMSE: %.4f\" % math.sqrt(mean_squared_error(y_test, predictx)))\n",
        "print(\"MAE: %.4f\" % mean_absolute_error(y_test, predictx))\n",
        "print(\"R2: %.4f\" % r2_score(y_test, predictx))\n",
        "#print(xgbr.score(X_train, y_train))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(xgbr, X_train, y_train, \n",
        "                         scoring='r2', cv=10,n_jobs=-1)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "r3kSC2Vb4R6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xdf = pd.DataFrame(X)\n",
        "Xf = list(Xdf.columns)\n",
        "Xdf = pd.DataFrame(X)\n",
        "Xf = list(Xdf.columns)\n",
        "# Get numerical feature importances\n",
        "importances = list(xgbr.feature_importances_)\n",
        "# List of tuples with variable and importance\n",
        "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(Xf, importances)]\n",
        "# Sort the feature importances by most important first\n",
        "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
        "# Print out the feature and importances \n",
        "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
      ],
      "metadata": {
        "id": "axhmzoMM4Vfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### xgb plots"
      ],
      "metadata": {
        "id": "MivsA36-F_P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scatter plot to show predicted and Observed values\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictx, c='crimson')\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "p1 = max(max(predictx), max(y_test))\n",
        "p2 = min(min(predictx), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-UT00r4FkusQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vcomb\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(y_test, predictx, c='crimson')\n",
        "\n",
        "\n",
        "p1 = max(max(predictx), max(y_test))\n",
        "p2 = min(min(predictx), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('Observed', fontsize=12)\n",
        "plt.ylabel('Predicted', fontsize=12)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uVhrWkYvxv2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FwN5oQ8Valzz",
        "3BWdbgujbiku",
        "sPNNqTk57k4S",
        "mm8fTYm_p9PF",
        "cy2KqVvbTXzk",
        "6Pm-RE4EThm0",
        "q7Rvxc0OXt_R",
        "ikbjCk0ZAv6c",
        "1ibp2lf3RhcD",
        "H1xWLhBJbamu",
        "kZ7ZZzlakDZu",
        "f-jB73Y4xB1d",
        "-YBIWgtWxCWG",
        "Y935VfW_xDEO",
        "qGRXrGy3xEAa",
        "b0R5cjjIxE1U",
        "NeOz158ExFpp",
        "6F84nfMZxGYG",
        "mfbB3xZTxHA8",
        "v6trKZUfI9pD",
        "AWsKhxP0NE6_",
        "l5UrwIbFNPky",
        "MivsA36-F_P8",
        "xlhjA2YwuBKU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}